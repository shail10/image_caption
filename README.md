# 
# IMAGE CAPTIONING USING TF AND KERAS

In this project I've attempted to predict caption off of images using the Flicker8K Dataset.
CNN + LSTM was used to first extract the features of Images and then running it through LSTM layers for caption prediction.

Read through [this paper](https://cs.stanford.edu/people/karpathy/main.pdf)
for further reference.

In this project I've used [InceptionV3](https://arxiv.org/abs/1512.00567) model for feature extraction and [GloVe](https://nlp.stanford.edu/projects/glove/) for word embedding.

## RUNNING THE MODEL 
You can either run the python notebook directly in google colab in GPU environment.

Or, clone the repository(or, download), download all the requirements.txt file and run the prediction.py file.

Note - You need to download the Flicker8K dataset and add the main folder to the repo in order to display the images whose caption you are prdicting.

## MODEL SUMMARY

## Screenshots

![App Screenshot](https://via.placeholder.com/468x300?text=App+Screenshot+Here)

  
